{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras as k\n",
    "from keras.layers import Merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\n",
    "from keras.callbacks import History\n",
    "from keras.layers import Activation\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import Adam\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage import rotate as rot\n",
    "np.random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              band_1  \\\n",
      "0  [-27.878360999999998, -27.15416, -28.668615, -...   \n",
      "1  [-12.242375, -14.920304999999999, -14.920363, ...   \n",
      "2  [-24.603676, -24.603714, -24.871029, -23.15277...   \n",
      "3  [-22.454607, -23.082819, -23.998013, -23.99805...   \n",
      "4  [-26.006956, -23.164886, -23.164886, -26.89116...   \n",
      "\n",
      "                                              band_2        id inc_angle  \\\n",
      "0  [-27.154118, -29.537888, -31.0306, -32.190483,...  dfd5f913   43.9239   \n",
      "1  [-31.506321, -27.984554, -26.645678, -23.76760...  e25388fd   38.1562   \n",
      "2  [-24.870956, -24.092632, -20.653963, -19.41104...  58b2aaa0   45.2859   \n",
      "3  [-27.889421, -27.519794, -27.165262, -29.10350...  4cfc3a18   43.8306   \n",
      "4  [-27.206915, -30.259186, -30.259186, -23.16495...  271f93f4   35.6256   \n",
      "\n",
      "   is_iceberg  \n",
      "0           0  \n",
      "1           0  \n",
      "2           1  \n",
      "3           0  \n",
      "4           0  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1604, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '../data/train.json'\n",
    "train = pd.read_json(file_path)\n",
    "\n",
    "print(train.head())\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train[train['inc_angle'] == 'na'].count()\n",
    "train.inc_angle = train.inc_angle.map(lambda x: 0.0 if x == 'na' else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform (df):\n",
    "    images = []\n",
    "    for i, row in df.iterrows():\n",
    "        band_1 = np.array(row['band_1']).reshape(75,75)\n",
    "        band_2 = np.array(row['band_2']).reshape(75,75)\n",
    "        band_3 = band_1 + band_2\n",
    "        \n",
    "        band_1_norm = (band_1 - band_1.mean()) / (band_1.max() - band_1.min())\n",
    "        band_2_norm = (band_2 - band_2. mean()) / (band_2.max() - band_2.min())\n",
    "        band_3_norm = (band_3 - band_3.mean()) / (band_3.max() - band_3.min())\n",
    "        \n",
    "        images.append(np.dstack((band_1_norm, band_2_norm, band_3_norm)))\n",
    "    \n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def augment(images):\n",
    "    image_mirror_lr = []\n",
    "    image_mirror_ud = []\n",
    "    image_rotate = []\n",
    "    for i in range(0,images.shape[0]):\n",
    "        band_1 = images[i,:,:,0]\n",
    "        band_2 = images[i,:,:,1]\n",
    "        band_3 = images[i,:,:,2]\n",
    "            \n",
    "        # mirror left-right\n",
    "        band_1_mirror_lr = np.fliplr(band_1)\n",
    "        band_2_mirror_lr = np.fliplr(band_2)\n",
    "        band_3_mirror_lr = np.fliplr(band_3)\n",
    "        image_mirror_lr.append(np.dstack((band_1_mirror_lr, band_2_mirror_lr, band_3_mirror_lr)))\n",
    "        \n",
    "        # mirror up-down\n",
    "        band_1_mirror_ud = np.flipud(band_1)\n",
    "        band_2_mirror_ud = np.flipud(band_2)\n",
    "        band_3_mirror_ud = np.flipud(band_3)\n",
    "        image_mirror_ud.append(np.dstack((band_1_mirror_ud, band_2_mirror_ud, band_3_mirror_ud)))\n",
    "        \n",
    "        #rotate \n",
    "        band_1_rotate = rot(band_1, 30, reshape=False)\n",
    "        band_2_rotate = rot(band_2, 30, reshape=False)\n",
    "        band_3_rotate = rot(band_3, 30, reshape=False)\n",
    "        image_rotate.append(np.dstack((band_1_rotate, band_2_rotate, band_3_rotate)))\n",
    "        \n",
    "    mirrorlr = np.array(image_mirror_lr)\n",
    "    mirrorud = np.array(image_mirror_ud)\n",
    "    rotated = np.array(image_rotate)\n",
    "    images = np.concatenate((images, mirrorlr, mirrorud, rotated))\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1471,)\n",
      "(5884, 75, 75, 3)\n",
      "(5884,)\n"
     ]
    }
   ],
   "source": [
    "train_x = transform(train)\n",
    "train_y = np.array(train ['is_iceberg'])\n",
    "\n",
    "indx_tr = np.where(train.inc_angle > 0)\n",
    "print (indx_tr[0].shape)\n",
    "\n",
    "train_y = train_y[indx_tr[0]]\n",
    "train_x = train_x[indx_tr[0], ...]\n",
    "\n",
    "train_x = augment(train_x)\n",
    "train_y = np.concatenate((train_y,train_y, train_y, train_y))\n",
    "\n",
    "print (train_x.shape)\n",
    "print (train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 73, 73, 64)        1792      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 73, 73, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 73, 73, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 34, 34, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 34, 34, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 15, 15, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 5, 5, 64)          73792     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 5, 5, 64)          256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 564,801\n",
      "Trainable params: 562,497\n",
      "Non-trainable params: 2,304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = k.models.Sequential()\n",
    "\n",
    "model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3,3), input_shape=(75,75,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(k.layers.convolutional.MaxPooling2D(pool_size=(3,3), strides=(2,2)))\n",
    "model.add(k.layers.Dropout(0.2))\n",
    "\n",
    "model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(k.layers.Dropout(0.2))\n",
    "\n",
    "model.add(k.layers.convolutional.Conv2D(128, kernel_size=(3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(k.layers.Dropout(0.3))\n",
    "\n",
    "model.add(k.layers.convolutional.Conv2D(64, kernel_size=(3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(k.layers.convolutional.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(k.layers.Dropout(0.3))\n",
    "\n",
    "model.add(k.layers.Flatten())\n",
    "\n",
    "model.add(k.layers.Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(k.layers.Dropout(0.2))\n",
    "\n",
    "model.add(k.layers.Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(k.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(k.layers.Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "mypotim=Adam(lr=0.01, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5295 samples, validate on 589 samples\n",
      "Epoch 1/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.5188 - acc: 0.7662Epoch 00001: val_loss improved from inf to 7.93590, saving model to weights.best.hdf5\n",
      "5295/5295 [==============================] - 129s 24ms/step - loss: 0.5159 - acc: 0.7679 - val_loss: 7.9359 - val_acc: 0.5076\n",
      "Epoch 2/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.3270 - acc: 0.8508Epoch 00002: val_loss did not improve\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.3263 - acc: 0.8510 - val_loss: 7.9359 - val_acc: 0.5076\n",
      "Epoch 3/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.2823 - acc: 0.8777Epoch 00003: val_loss did not improve\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.2810 - acc: 0.8782 - val_loss: 7.9359 - val_acc: 0.5076\n",
      "Epoch 4/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.2685 - acc: 0.8859Epoch 00004: val_loss improved from 7.93590 to 5.47711, saving model to weights.best.hdf5\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.2678 - acc: 0.8861 - val_loss: 5.4771 - val_acc: 0.5076\n",
      "Epoch 5/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.2386 - acc: 0.8963Epoch 00005: val_loss improved from 5.47711 to 2.95932, saving model to weights.best.hdf5\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.2387 - acc: 0.8963 - val_loss: 2.9593 - val_acc: 0.5076\n",
      "Epoch 6/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.2375 - acc: 0.9021Epoch 00006: val_loss improved from 2.95932 to 0.27113, saving model to weights.best.hdf5\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.2376 - acc: 0.9016 - val_loss: 0.2711 - val_acc: 0.8778\n",
      "Epoch 7/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.2221 - acc: 0.9074Epoch 00007: val_loss did not improve\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.2217 - acc: 0.9073 - val_loss: 0.9303 - val_acc: 0.7029\n",
      "Epoch 8/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.2129 - acc: 0.9133Epoch 00008: val_loss did not improve\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.2121 - acc: 0.9137 - val_loss: 0.7478 - val_acc: 0.7063\n",
      "Epoch 9/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.2047 - acc: 0.9110Epoch 00009: val_loss did not improve\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.2055 - acc: 0.9109 - val_loss: 0.3377 - val_acc: 0.8625\n",
      "Epoch 10/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1999 - acc: 0.9181Epoch 00010: val_loss did not improve\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.1999 - acc: 0.9180 - val_loss: 0.8672 - val_acc: 0.7182\n",
      "Epoch 11/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1959 - acc: 0.9202Epoch 00011: val_loss did not improve\n",
      "5295/5295 [==============================] - 126s 24ms/step - loss: 0.1954 - acc: 0.9203 - val_loss: 0.3931 - val_acc: 0.8421\n",
      "Epoch 12/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1941 - acc: 0.9213Epoch 00012: val_loss did not improve\n",
      "5295/5295 [==============================] - 136s 26ms/step - loss: 0.1958 - acc: 0.9205 - val_loss: 0.5508 - val_acc: 0.8353\n",
      "Epoch 13/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1853 - acc: 0.9221Epoch 00013: val_loss did not improve\n",
      "5295/5295 [==============================] - 139s 26ms/step - loss: 0.1851 - acc: 0.9224 - val_loss: 0.4078 - val_acc: 0.8472\n",
      "Epoch 14/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1664 - acc: 0.9373Epoch 00014: val_loss did not improve\n",
      "5295/5295 [==============================] - 140s 27ms/step - loss: 0.1675 - acc: 0.9371 - val_loss: 0.5441 - val_acc: 0.8608\n",
      "Epoch 15/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1513 - acc: 0.9415Epoch 00015: val_loss did not improve\n",
      "5295/5295 [==============================] - 139s 26ms/step - loss: 0.1525 - acc: 0.9413 - val_loss: 0.3791 - val_acc: 0.8676\n",
      "Epoch 16/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1463 - acc: 0.9404Epoch 00016: val_loss improved from 0.27113 to 0.24782, saving model to weights.best.hdf5\n",
      "5295/5295 [==============================] - 142s 27ms/step - loss: 0.1457 - acc: 0.9405 - val_loss: 0.2478 - val_acc: 0.9083\n",
      "Epoch 17/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1417 - acc: 0.9428Epoch 00017: val_loss did not improve\n",
      "5295/5295 [==============================] - 141s 27ms/step - loss: 0.1417 - acc: 0.9428 - val_loss: 0.5042 - val_acc: 0.8591\n",
      "Epoch 18/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1424 - acc: 0.9421Epoch 00018: val_loss did not improve\n",
      "5295/5295 [==============================] - 138s 26ms/step - loss: 0.1423 - acc: 0.9422 - val_loss: 0.4390 - val_acc: 0.8659\n",
      "Epoch 19/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1436 - acc: 0.9457Epoch 00019: val_loss did not improve\n",
      "5295/5295 [==============================] - 141s 27ms/step - loss: 0.1434 - acc: 0.9456 - val_loss: 0.6942 - val_acc: 0.7623\n",
      "Epoch 20/20\n",
      "5248/5295 [============================>.] - ETA: 1s - loss: 0.1337 - acc: 0.9489Epoch 00020: val_loss did not improve\n",
      "5295/5295 [==============================] - 136s 26ms/step - loss: 0.1331 - acc: 0.9492 - val_loss: 0.3157 - val_acc: 0.8812\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 0, mode= 'min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor = 0.1, patience = 7, verbose =1, \n",
    "                                   epsilon = 1e-4, mode='min', min_lr = 0.0001)\n",
    "model_filepath='weights.best.hdf5'\n",
    "checkpoint = ModelCheckpoint(model_filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [early_stopping, checkpoint]\n",
    "history = model.fit(train_x, train_y, batch_size = batch_size, epochs =20, verbose =1, validation_split = 0.1, \n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'acc', 'loss', 'val_acc'])\n"
     ]
    }
   ],
   "source": [
    "print (history.history.keys())\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'],loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('weights.best.hdf5')\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer = mypotim, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8424, 75, 75, 3)\n"
     ]
    }
   ],
   "source": [
    "test_file = '../data/test.json'\n",
    "test = pd.read_json(test_file)\n",
    "test.inc_angle = test.inc_angle.replace('na',0)\n",
    "test_x = transform(test)\n",
    "print (test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8424/8424 [==============================] - 80s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_test = loaded_model.predict(test_x, verbose=1)\n",
    "submission = pd.DataFrame({'id': test[\"id\"], 'is_iceberg': pred_test.reshape((pred_test.shape[0]))})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
